{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dataset: (10000, 6)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate 10,000 entries and 2 random features\n",
    "independent_features = np.random.rand(10000, 2)\n",
    "\n",
    "# Create features based on polynomial combinations\n",
    "feature_cos = np.cos(independent_features[:, 0] + independent_features[:, 1])\n",
    "feature_tan = np.tan(independent_features[:, 0] * independent_features[:, 1])\n",
    "feature_sin = np.sin(independent_features[:, 0]**2 + independent_features[:, 1]**2)\n",
    "\n",
    "# Combine all features into a single array\n",
    "dataset = np.column_stack((independent_features, feature_cos, feature_tan, feature_sin))\n",
    "\n",
    "# Add a final feature as a simple polynomial combination\n",
    "final_feature_continuous = (\n",
    "    2 * dataset[:, 0] +\n",
    "    3 * dataset[:, 1] +\n",
    "    0.5 * dataset[:, 2] +\n",
    "    1.5 * dataset[:, 3] +\n",
    "    0.8 * dataset[:, 4] +\n",
    "    np.random.normal(scale=0.1, size=(10000,))\n",
    ")\n",
    "\n",
    "# Convert continuous values to binary (0 or 1) using a threshold\n",
    "threshold = np.median(final_feature_continuous)\n",
    "final_feature_binary = (final_feature_continuous > threshold).astype(int)\n",
    "\n",
    "# Add the final feature to the dataset\n",
    "dataset = np.column_stack((dataset, final_feature_binary))\n",
    "\n",
    "# Print the shape of the generated dataset\n",
    "print(\"Shape of the dataset:\", dataset.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.37454012 0.95071431 0.24308201 0.37193478 0.86449157 1.        ]\n",
      " [0.73199394 0.59865848 0.2378424  0.46860099 0.77971287 1.        ]\n",
      " [0.15601864 0.15599452 0.95171751 0.02434286 0.04865689 0.        ]\n",
      " [0.05808361 0.86617615 0.6024256  0.05035313 0.68429381 0.        ]\n",
      " [0.60111501 0.70807258 0.25863489 0.4533461  0.7596053  1.        ]\n",
      " [0.02058449 0.96990985 0.54827651 0.01996776 0.80823514 1.        ]\n",
      " [0.83244264 0.21233911 0.50209068 0.17862434 0.67284562 0.        ]\n",
      " [0.18182497 0.18340451 0.93404182 0.03335989 0.06664809 0.        ]\n",
      " [0.30424224 0.52475643 0.67561433 0.16102352 0.35968722 0.        ]\n",
      " [0.43194502 0.29122914 0.74970895 0.12646275 0.2680717  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(dataset[:10,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt('/home/er647/projects/feature-wise-active-learning/data/SyntheticData/simple_synthetic_dataset.csv', dataset, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Dataset:\n",
      "[[0.37454012 0.95071431 0.24308201 0.37193478 0.86449157 1.        ]\n",
      " [0.73199394 0.59865848 0.2378424  0.46860099 0.77971287 1.        ]\n",
      " [0.15601864 0.15599452 0.95171751 0.02434286 0.04865689 0.        ]\n",
      " [0.05808361 0.86617615 0.6024256  0.05035313 0.68429381 0.        ]\n",
      " [0.60111501 0.70807258 0.25863489 0.4533461  0.7596053  1.        ]\n",
      " [0.02058449 0.96990985 0.54827651 0.01996776 0.80823514 1.        ]\n",
      " [0.83244264 0.21233911 0.50209068 0.17862434 0.67284562 0.        ]\n",
      " [0.18182497 0.18340451 0.93404182 0.03335989 0.06664809 0.        ]\n",
      " [0.30424224 0.52475643 0.67561433 0.16102352 0.35968722 0.        ]\n",
      " [0.43194502 0.29122914 0.74970895 0.12646275 0.2680717  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Specify the path to the CSV file\n",
    "csv_file_path = '/home/er647/projects/feature-wise-active-learning/data/SyntheticData/simple_synthetic_dataset.csv'  # Replace with the actual path to your CSV file\n",
    "\n",
    "# Load the dataset from the CSV file\n",
    "loaded_dataset = np.loadtxt(csv_file_path, delimiter=\",\")\n",
    "\n",
    "# Print the loaded dataset\n",
    "print(\"Loaded Dataset:\")\n",
    "print(loaded_dataset[:10,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dataset: (10000, 6)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate 10,000 entries and 2 random features\n",
    "independent_features = np.random.rand(10000, 2)\n",
    "\n",
    "# Create features based on polynomial combinations\n",
    "feature_1 = 1*independent_features[:, 0] + 2*independent_features[:, 1]\n",
    "feature_2 = 3*independent_features[:, 0] * 4*independent_features[:, 1]\n",
    "feature_3 = 5*independent_features[:, 0] + 6*independent_features[:, 1]\n",
    "\n",
    "# Combine all features into a single array\n",
    "dataset = np.column_stack((independent_features, feature_1, feature_2, feature_3))\n",
    "\n",
    "# Add a final feature as a simple polynomial combination\n",
    "final_feature_continuous = (\n",
    "    2 * dataset[:, 0] +\n",
    "    3 * dataset[:, 1] +\n",
    "    0.5 * dataset[:, 2] +\n",
    "    1.5 * dataset[:, 3] +\n",
    "    0.8 * dataset[:, 4] +\n",
    "    np.random.normal(scale=0.1, size=(10000,))\n",
    ")\n",
    "\n",
    "# Convert continuous values to binary (0 or 1) using a threshold\n",
    "threshold = np.median(final_feature_continuous)\n",
    "final_feature_binary = (final_feature_continuous > threshold).astype(int)\n",
    "\n",
    "# Add the final feature to the dataset\n",
    "dataset = np.column_stack((dataset, final_feature_binary))\n",
    "\n",
    "# Print the shape of the generated dataset\n",
    "print(\"Shape of the dataset:\", dataset.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt('/home/er647/projects/feature-wise-active-learning/data/SyntheticData/very_simple_synthetic_dataset.csv', dataset, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Dataset:\n",
      "[[0.37454012 0.95071431 2.27596873 4.27296779 7.57698643 1.        ]\n",
      " [0.73199394 0.59865848 1.92931091 5.2585726  7.25192061 1.        ]\n",
      " [0.15601864 0.15599452 0.46800768 0.29205664 1.71606032 0.        ]\n",
      " [0.05808361 0.86617615 1.7904359  0.60372767 5.48747494 0.        ]\n",
      " [0.60111501 0.70807258 2.01726017 5.10759667 7.25401053 1.        ]\n",
      " [0.02058449 0.96990985 1.9604042  0.23958125 5.92238158 0.        ]\n",
      " [0.83244264 0.21233911 1.25712086 2.12112156 5.43624787 0.        ]\n",
      " [0.18182497 0.18340451 0.54863399 0.40017023 2.0095519  0.        ]\n",
      " [0.30424224 0.52475643 1.35375511 1.91583689 4.6697498  0.        ]\n",
      " [0.43194502 0.29122914 1.0144033  1.50953972 3.90709993 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Specify the path to the CSV file\n",
    "csv_file_path = '/home/er647/projects/feature-wise-active-learning/data/SyntheticData/very_simple_synthetic_dataset.csv'  # Replace with the actual path to your CSV file\n",
    "\n",
    "# Load the dataset from the CSV file\n",
    "loaded_dataset = np.loadtxt(csv_file_path, delimiter=\",\")\n",
    "\n",
    "# Print the loaded dataset\n",
    "print(\"Loaded Dataset:\")\n",
    "print(loaded_dataset[:10,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate synthetic datasets with 5 independent features and 6 derived features\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def generate_modified_synthetic_data(n_samples=10000):\n",
    "    # Generate 5-dimensional independent input features from a Gaussian distribution\n",
    "    X_independent = np.random.normal(0, 1, (n_samples, 5))\n",
    "\n",
    "    # Initialize an array to hold all 11 features\n",
    "    X1 = np.zeros((n_samples, 11))\n",
    "\n",
    "    # The first 5 features are independent\n",
    "    X1[:, :5] = X_independent\n",
    "\n",
    "    # Syn1: Derived features based on exp(X1 * X2) with unique coefficients\n",
    "    X1[:, 5] = np.exp(0.1 * X1[:, 0] * X1[:, 1])\n",
    "    X1[:, 6] = np.exp(0.2 * X1[:, 0] * X1[:, 1])\n",
    "    X1[:, 7] = np.exp(0.3 * X1[:, 0] * X1[:, 1])\n",
    "    X1[:, 8] = np.exp(0.4 * X1[:, 0] * X1[:, 1])\n",
    "    X1[:, 9] = np.exp(0.5 * X1[:, 0] * X1[:, 1])\n",
    "    X1[:, 10] = np.exp(0.6 * X1[:, 0] * X1[:, 1])\n",
    "    y_Syn1 = np.random.binomial(1, 1 / (1 + np.exp(-np.exp(X1[:, 0] * X1[:, 1]))))\n",
    "\n",
    "\n",
    "    X2 = np.zeros((n_samples, 11))\n",
    "\n",
    "    # The first 5 features are independent\n",
    "    X2[:, :5] = X_independent\n",
    "    \n",
    "    # Syn2: Derived features based on exp(sum(X3^2 to X6^2) - 4) with unique coefficients\n",
    "    derived_features_Syn2 = np.exp(0.1 * np.sum(X_independent[:, 2:5] ** 2, axis=1) - 4)\n",
    "    for i in range(5, 11):\n",
    "        X2[:, i] = (i - 4) * derived_features_Syn2\n",
    "    y_Syn2 = np.random.binomial(1, 1 / (1 + np.exp(-np.exp(np.sum(X2[:, 2:6] ** 2, axis=1) - 4))))\n",
    "\n",
    "    X3 = np.zeros((n_samples, 11))\n",
    "\n",
    "    # The first 5 features are independent\n",
    "    X3[:, :5] = X_independent\n",
    "    \n",
    "    # Syn3: Derived features based on -10 * sin(2 * X7) + 2 * abs(X8) + X9 + exp(-X10) with unique coefficients\n",
    "    derived_features_Syn3 = -10 * np.sin(2 * X_independent[:, 4]) + 2 * np.abs(X_independent[:, 3]) + X_independent[:, 2] + np.exp(-X_independent[:, 1])\n",
    "    for i in range(5, 11):\n",
    "        X3[:, i] = (i - 4) * derived_features_Syn3\n",
    "    y_Syn3 = np.random.binomial(1, 1 / (1 + np.exp(-(-10 * np.sin(2 * X3[:, 6]) + 2 * np.abs(X3[:, 7]) + X3[:, 8] + np.exp(-X3[:, 9])))))\n",
    "\n",
    "    return np.column_stack((X1, y_Syn1)), np.column_stack((X2, y_Syn2)), np.column_stack((X3, y_Syn3))\n",
    "    # return (X1, y_Syn1), (X2, y_Syn2), (X3, y_Syn3)\n",
    "\n",
    "# Generate the modified datasets\n",
    "EID_syn, SSED_syn, TPD_syn = generate_modified_synthetic_data()\n",
    "\n",
    "# Display the shapes of the modified datasets\n",
    "# Modified_Syn1[0].shape, Modified_Syn1[1].shape, Modified_Syn2[0].shape, Modified_Syn2[1].shape, Modified_Syn3[0].shape, Modified_Syn3[1].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('/home/er647/projects/feature-wise-active-learning/data/SyntheticData/exponential_interaction_synthetic_dataset.csv', EID_syn, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('/home/er647/projects/feature-wise-active-learning/data/SyntheticData/summed_squares_exponential_synthetic_dataset.csv', SSED_syn, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('/home/er647/projects/feature-wise-active-learning/data/SyntheticData/trigonometric_polynomial_synthetic_dataset.csv', TPD_syn, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 0., ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EID_syn[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FWAL_kernel",
   "language": "python",
   "name": "fwal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
